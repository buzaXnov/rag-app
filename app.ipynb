{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import requests\n",
    "from flask import Flask, jsonify, request\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms.llamafile import Llamafile\n",
    "from langchain_community.embeddings import LlamafileEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Llamafile(base_url='http://localhost:8081', temperature=0.7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamafile = Llamafile(base_url=\"http://localhost:8081\", n_predict=400, temperature=0.7)\n",
    "llamafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A man walks into a bar and asks for a drink. The bartender looks at him, rolls his eyes, and tells him to go away. A few minutes later, the same man walks in again.\n",
      "\"Oh, I'm sorry,\" says the bartender apologetically. \"I thought you were going to ask for something different.\"\n",
      "The man takes a sip of beer. \"No,\" he said, \"I want a shot of your best whiskey. Don't tell me, it's not on the menu.\"\n",
      "A woman walks into a bar and asks for a drink. The bartender looks at her, rolls his eyes, and tells her to go away. A few minutes later, the same woman walks in again.\n",
      "\"Oh, I'm sorry,\" says the bartender apologetically. \"I thought you were going to ask for something different.\"\n",
      "The woman takes a sip of beer. \"No,\" she said, \"I want a shot of your best scotch. Don't tell me, it's not on the menu.\"\n",
      "A man walks into a bar and asks for a drink. The bartender looks at him, rolls his eyes, and tells him to go away. A few minutes later, the same man walks in again.\n",
      "\"Oh, I'm sorry,\" says the bartender apologetically. \"I thought you were going to ask for something different.\"\n",
      "The man takes a sip of beer. \"No,\" he said, \"I want a shot of your best vodka. Don't tell me, it's not on the menu.\"\n",
      "A woman walks into a bar and asks for a drink. The bartender looks at her, rolls his eyes, and tells her to go away. A few minutes later, the same\n"
     ]
    }
   ],
   "source": [
    "print(llamafile.invoke(\"Hi. Can you tell me a joke?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamafileEmbeddings(base_url='http://localhost:8080', request_timeout=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llamafile_embedder = LlamafileEmbeddings(base_url=\"http://localhost:8080\")\n",
    "llamafile_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.007116288878023624,\n",
       " -0.018593888729810715,\n",
       " 0.05576428771018982,\n",
       " 0.024310659617185593,\n",
       " -0.051241591572761536]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a test document.\"\n",
    "query_result = llamafile_embedder.embed_query(text)\n",
    "query_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my loader\n",
    "\n",
    "# needed to install chardet here for the TextLoader encoding autodetect\n",
    "path = \"toy_data\"\n",
    "text_loader_kwargs={\"encoding\" : \"windows-1252\"} # autodetect_encoding': True, \n",
    "# NOTE: autoencoding could not find the encoding but a random stack-overflow search did; long live koPytok\n",
    "# Source: https://stackoverflow.com/questions/48067514/utf-8-codec-cant-decode-byte-0xa0-in-position-4276-invalid-start-byte\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path, glob=\"**/*.txt\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10, chunk_overlap=5)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.from_documents(documents=splits, embedding=llamafile_embedder)  # vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'toy_data/4.txt'}, page_content='triangles'),\n",
       " Document(metadata={'source': 'toy_data/3.txt'}, page_content='triangles'),\n",
       " Document(metadata={'source': 'toy_data/1.txt'}, page_content='squares.'),\n",
       " Document(metadata={'source': 'toy_data/2.txt'}, page_content='circles.')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.index.ntotal\n",
    "vector_store.similarity_search(query=\"Triangles?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vector_store.save_local(\"faiss_index\")\n",
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(\"Who likes circles?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'toy_data/2.txt'}, page_content='circles.')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(vector_store, FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7146921778e0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FAISS.load_local(\"faiss_index\", llamafile_embedder, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import json\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nthe current application. To solve this, set up an application context\nwith app.app_context(). See the documentation for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m top_k_docs \u001b[38;5;241m=\u001b[39m [doc_to_dict(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m top_k_docs]\n\u001b[1;32m     11\u001b[0m top_k_docs\n\u001b[0;32m---> 12\u001b[0m \u001b[43mjsonify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgenerated_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtop_k_documents\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_docs\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/rag-app/.venv/lib/python3.10/site-packages/flask/json/__init__.py:170\u001b[0m, in \u001b[0;36mjsonify\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjsonify\u001b[39m(\u001b[38;5;241m*\u001b[39margs: t\u001b[38;5;241m.\u001b[39mAny, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: t\u001b[38;5;241m.\u001b[39mAny) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Response:\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Serialize the given arguments as JSON, and return a\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    :class:`~flask.Response` object with the ``application/json``\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m    mimetype. A dict or list returned from a view will be converted to a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 0.2\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcurrent_app\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mresponse(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Documents/rag-app/.venv/lib/python3.10/site-packages/werkzeug/local.py:318\u001b[0m, in \u001b[0;36m_ProxyLookup.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_current_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/rag-app/.venv/lib/python3.10/site-packages/werkzeug/local.py:519\u001b[0m, in \u001b[0;36mLocalProxy.__init__.<locals>._get_current_object\u001b[0;34m()\u001b[0m\n\u001b[1;32m    517\u001b[0m     obj \u001b[38;5;241m=\u001b[39m local\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(unbound_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m get_name(obj)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Working outside of application context.\n\nThis typically means that you attempted to use functionality that needed\nthe current application. To solve this, set up an application context\nwith app.app_context(). See the documentation for more information."
     ]
    }
   ],
   "source": [
    "generated_text = \"paka paka\"\n",
    "top_k_docs = [Document(page_content=\"1\"), Document(page_content=\"2\")]\n",
    "\n",
    "def doc_to_dict(doc):\n",
    "    return {\n",
    "        'page_content': doc.page_content,\n",
    "        'metadata': doc.metadata\n",
    "    }\n",
    "\n",
    "top_k_docs = [doc_to_dict(doc) for doc in top_k_docs]\n",
    "top_k_docs\n",
    "jsonify({\n",
    "    'generated_text': generated_text,\n",
    "    'top_k_documents': top_k_docs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PromptTemplate, ChatPromptTemplate\n\u001b[0;32m----> 3\u001b[0m \u001b[43mChatPromptTemplate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[1;32m      4\u001b[0m     [\n\u001b[1;32m      5\u001b[0m         (\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know. Use three sentences maximum and keep the answer concise.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m         ),\n\u001b[1;32m      8\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuestion: \u001b[39m\u001b[38;5;132;01m{question}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer:\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     11\u001b[0m     ]\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/rag-app/.venv/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m~/Documents/rag-app/.venv/lib/python3.10/site-packages/pydantic/v1/main.py:1048\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m validator \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39m__pre_root_validators__:\n\u001b[1;32m   1047\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1048\u001b[0m         input_data \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1049\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}, \u001b[38;5;28mset\u001b[39m(), ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], cls_)\n",
      "File \u001b[0;32m~/Documents/rag-app/.venv/lib/python3.10/site-packages/langchain_core/prompts/chat.py:970\u001b[0m, in \u001b[0;36mChatPromptTemplate.validate_input_variables\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;129m@root_validator\u001b[39m(pre\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_input_variables\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[1;32m    956\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input variables.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \n\u001b[1;32m    958\u001b[0m \u001b[38;5;124;03m    If input_variables is not set, it will be set to the union of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;124;03m        ValueError: If input variables do not match.\u001b[39;00m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 970\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    971\u001b[0m     input_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m    972\u001b[0m     optional_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'messages'"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "ChatPromptTemplate().from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"\n",
    "        ),\n",
    "        (\"Question: {question} \"),\n",
    "        (\"Context: {context} \"),\n",
    "        (\"Answer:\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.prompts import HumanMessagePromptTemplate\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import LlamafileEmbeddings\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "web_paths = (\n",
    "    \"https://lilianweng.github.io/posts/2020-10-29-odqa/\",\n",
    "    \"https://lilianweng.github.io/posts/2020-08-06-nas/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-01-10-inference-optimization/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-09-08-ntk/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-06-09-vlm/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-04-15-data-gen/\",\n",
    "    \"https://lilianweng.github.io/posts/2022-02-20-active-learning/\",\n",
    "    \"https://lilianweng.github.io/posts/2021-12-05-semi-supervised/\",\n",
    "    \"https://lilianweng.github.io/posts/2021-09-25-train-large/\",\n",
    "    \"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\",\n",
    "    \"https://lilianweng.github.io/posts/2021-05-31-contrastive/\",\n",
    "    \"https://lilianweng.github.io/posts/2021-03-21-lm-toxicity/\",\n",
    "    \"https://lilianweng.github.io/posts/2021-01-02-controllable-text-generation/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    ")\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=web_paths,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "embeddings_model = LlamafileEmbeddings(base_url=\"http://localhost:8080\")\n",
    "vector_store = FAISS.from_documents(\n",
    "    documents=splits, embedding=embeddings_model\n",
    ")\n",
    "vector_store.save_local(\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_url='http://localhost:8080' request_timeout=None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from config import Config\n",
    "from services.embedder import Embedder\n",
    "from services.generator import Generator\n",
    "from services.retriever import Retriever\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "embedder = Embedder(\"http://localhost:8080\")\n",
    "generator = Generator(\"http://localhost:8081\", temperature=0.7)\n",
    "\n",
    "# Build or load FAISS index\n",
    "if os.path.exists(Config.VECTOR_STORE_PATH):\n",
    "    vector_store = embedder.load_vector_store(Config.VECTOR_STORE_PATH)\n",
    "else:\n",
    "    embedder.create_vector_store(\n",
    "        txt_path=Config.DATA_DIR,\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    embedder.save_vector_store(Config.VECTOR_STORE_PATH)\n",
    "\n",
    "retriever = Retriever(embedder.vector_store)\n",
    "\n",
    "# user_query = \"What is task decompposition?\"\n",
    "# top_k_docs = retriever.retrieve_documents(user_query)\n",
    "\n",
    "# prompt = ChatPromptTemplate(\n",
    "#     input_variables=[\"context\", \"question\"],\n",
    "#     messages=[\n",
    "#         HumanMessagePromptTemplate(\n",
    "#             prompt=PromptTemplate(\n",
    "#                 input_variables=[\"context\", \"question\"],\n",
    "#                 template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
    "#             )\n",
    "#         )\n",
    "#     ],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\"context\": retriever.retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | generator.generator\n",
    "    | StrOutputParser()\n",
    ")\n",
    "user_query = \"What is task decompposition?\"\n",
    "\n",
    "generated_text = chain.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\"First, you need to identify the subgoals. Then, you can decompose them into smaller tasks. The user input text might look like this: A game with MCV (Mario, Cave, Villiage, etc.). Keyboard controls are essential for keyboard-only users. To achieve that goal, you can perform the following steps:\"\n",
      "[{\"task\": \"Keyboard Control\", \"id\": 1}, {\"task\": \"MVC components\", \"id\": 2}, {\"task\": \"Keyboard control\", \"id\": 3}]\n",
      "Now, based on this decomposition, you generate multiple tasks such as:\n",
      "- Task 1: Keyboard Control (“Write instructions for keyboard controls in the game. For example, press A to move left, B to move right, and so on.”)\n",
      "- Task 2: MVC Components (“Develop a set of classes that define MVC components. For example, a class named “MVCComponent” for each component in the MVC model. ”)\n",
      "- Task 3: Keyboard Control (“Write a function that performs keyboard controls for the game. This function should take in a key code and return the corresponding action to perform on the keyboard.\")\n",
      "The user inputs are then translated into these tasks, which the agent can plan ahead and complete as necessary.\n",
      "\n",
      "Fig. 2. Schematic representation of task decomposition in LLM-powered autonomous agents.\n",
      "Component Two: Decomposition#\n",
      "Combining tasks leads to more complex problems that require different skill sets and are challenging for humans. Therefore, a system can perform task decompositions, which allows the agent to select the most appropriate task for each step of a complex problem.\n",
      "Task Decomposition#\n",
      "In this example, we have selected the \"Keyboard Control\" task from the \"MVC Components\" task decomposition in Fig. 1. The user input\n"
     ]
    }
   ],
   "source": [
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = FAISS.load_local(\n",
    "    \"faiss_index\", embedder.embeddings_model, allow_dangerous_deserialization=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-01-10-inference-optimization/'}, page_content='Check the previous post on large model training on different types of training parallelism and memory saving designs including CPU memory offloading. This post focuses on network compression techniques and architecture-specific improvement for transformer models.\\nDistillation#\\nKnowledge Distillation (KD; Hinton et al. 2015, Gou et al. 2020) is a straightforward way to build a smaller, cheaper model (“student model”) to speed up inference by transferring skills from a pre-trained expensive model (“teacher model”) into the student. There is no much restriction on how the student architecture should be constructed, except for a matched output space with the teacher in order to construct a proper learning objective.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-01-10-inference-optimization/'}, page_content='Fig. 1. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al. 2020)\\nGiven a dataset, a student model is trained to mimic outputs of a teacher via distillation loss. Usually a neural network has a softmax layer; For example, a LLM outputs a probability distribution over tokens. Let’s denote the logits layer right before softmax as $\\\\mathbf{z}_t$ and $\\\\mathbf{z}_s$ for teacher and student models, respectively. The distillation loss minimizes the difference between two softmax outputs with a high temperature $T$. When ground truth labels $\\\\mathbf{y}$ are known, we can combine it with a supervised learning objective between ground truth and the student’s soft logits using e.g. cross-entropy.\\n\\n$$\\n\\\\mathcal{L}_\\\\text{KD} = \\\\mathcal{L}_\\\\text{distll}(\\\\text{softmax}(\\\\mathbf{z}_t, T), \\\\text{softmax}(\\\\mathbf{z}_s, T)) + \\\\lambda\\\\mathcal{L}_\\\\text{CE}(\\\\mathbf{y}, \\\\mathbf{z}_s)\\n$$')]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "docs = ret.invoke(\"What is Knowledge distillation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fig. 2. The example of distillation on a transformer model, with the teacher and student output spaces. (Image source: Gou et al. 2020)\n",
      "\n",
      "Transformers#\n",
      "Besides transformer models, there are various other types of neural networks that have been shown to work well for language tasks. Many of these architectures have seen significant improvements in performance, particularly on text classification problems. Here we’ll focus on the BERT and RoBERTa models, but many other variants exist.\n",
      "\n",
      "Transformers consist of a self-attention module at the beginning of each layer, followed by a feedforward network for downstream tasks. The self-attention mechanism allows the model to attend over the input tokens based on their position in the sequence (i.e., learn how words are related to one another), while the feedforward network learns features from the entire sequence using hidden states.\n",
      "\n",
      "In this section, we’ll discuss a few architectural details of transformer models and how they can be optimized for large model training on different types of parallelism and memory-saving designs.\n",
      "\n",
      "Bert#\n",
      "The BERT model is an excellent example of a Transformer architecture that has seen significant improvements in performance on various downstream tasks such as text classification, question answering, and machine translation. The original version was released by the Google Research team (Kreutzer et al., 2018). It’s based on a self-attention mechanism that allows the model to attend over the input tokens based on their position in the sequence.\n",
      "\n",
      "The Bert model has two main components: the BERT layer and the pooler output. The BERT layer consists of an encoder, which is composed of multiple layers with self-attention mechanisms. The encoder’s outputs are then fed through a feedforward network, which is composed of multiple hidden layers\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | generator.generator | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What is Knowledge distillation?\"\n",
    "docs = retriever.retrieve_documents(question)\n",
    "\n",
    "txt = chain.invoke(docs)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Check the previous post on large model training on different types of training parallelism and memory saving designs including CPU memory offloading. This post focuses on network compression techniques and architecture-specific improvement for transformer models.\\nDistillation#\\nKnowledge Distillation (KD; Hinton et al. 2015, Gou et al. 2020) is a straightforward way to build a smaller, cheaper model (“student model”) to speed up inference by transferring skills from a pre-trained expensive model (“teacher model”) into the student. There is no much restriction on how the student architecture should be constructed, except for a matched output space with the teacher in order to construct a proper learning objective.',\n",
       " 'Fig. 1. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al. 2020)\\nGiven a dataset, a student model is trained to mimic outputs of a teacher via distillation loss. Usually a neural network has a softmax layer; For example, a LLM outputs a probability distribution over tokens. Let’s denote the logits layer right before softmax as $\\\\mathbf{z}_t$ and $\\\\mathbf{z}_s$ for teacher and student models, respectively. The distillation loss minimizes the difference between two softmax outputs with a high temperature $T$. When ground truth labels $\\\\mathbf{y}$ are known, we can combine it with a supervised learning objective between ground truth and the student’s soft logits using e.g. cross-entropy.\\n\\n$$\\n\\\\mathcal{L}_\\\\text{KD} = \\\\mathcal{L}_\\\\text{distll}(\\\\text{softmax}(\\\\mathbf{z}_t, T), \\\\text{softmax}(\\\\mathbf{z}_s, T)) + \\\\lambda\\\\mathcal{L}_\\\\text{CE}(\\\\mathbf{y}, \\\\mathbf{z}_s)\\n$$']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "Knowledge distillation is a way to transfer the knowledge from an expensive teacher model into a cheaper student model. In this case, we assume that the output space of the teacher and student are matched (i.e., the softmax layer is the same). The goal is to minimize the difference between two logits outputs, where the ground truth labels $\\mathbf{y}$ are known. We can combine this with a supervised objective using e.g. Cross-entropy loss. In our case, we use the distillation loss, which combines the softmax output and the supervised loss.</s>\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate(\n",
    "            prompt=PromptTemplate(\n",
    "                input_variables=[\"context\", \"question\"],\n",
    "                template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "ret = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "docs = ret.invoke(\"What is Knowledge distillation?\")\n",
    "\n",
    "chain = (\n",
    "    {\"context\": ret | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | generator.generator\n",
    "    | StrOutputParser()\n",
    ")\n",
    "user_query = \"What is Knowledge distillation?\"\n",
    "\n",
    "generated_text = chain.invoke(user_query)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.65:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "127.0.0.1 - - [19/Jul/2024 17:39:04] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def doc_to_dict(doc):\n",
    "    \"\"\"\n",
    "    Objects of type Document are non serializable so this utils function was made\n",
    "    that turns a Document object into a dictionary.\n",
    "    \"\"\"\n",
    "    return {\"page_content\": doc.page_content, \"metadata\": doc.metadata}\n",
    "\n",
    "@app.route(\"/query\", methods=[\"POST\"])\n",
    "def query():\n",
    "    data = request.get_json()\n",
    "    user_query = data[\"query\"]\n",
    "    # top_k_docs = retriever.retrieve_documents(user_query)\n",
    "\n",
    "    prompt = ChatPromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        messages=[\n",
    "            HumanMessagePromptTemplate(\n",
    "                prompt=PromptTemplate(\n",
    "                    input_variables=[\"context\", \"question\"],\n",
    "                    template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\",\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    ret = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "    chain = (\n",
    "        {\"context\": ret | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | generator.generator\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "\n",
    "    docs = ret.invoke(user_query)\n",
    "    generated_text = chain.invoke(user_query)\n",
    "\n",
    "    return jsonify(\n",
    "        {\n",
    "            \"generated_text\": generated_text,\n",
    "            \"top_k_documents\": [doc_to_dict(doc) for doc in docs],\n",
    "        }\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.get_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-01-10-inference-optimization/'}, page_content='Check the previous post on large model training on different types of training parallelism and memory saving designs including CPU memory offloading. This post focuses on network compression techniques and architecture-specific improvement for transformer models.\\nDistillation#\\nKnowledge Distillation (KD; Hinton et al. 2015, Gou et al. 2020) is a straightforward way to build a smaller, cheaper model (“student model”) to speed up inference by transferring skills from a pre-trained expensive model (“teacher model”) into the student. There is no much restriction on how the student architecture should be constructed, except for a matched output space with the teacher in order to construct a proper learning objective.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-01-10-inference-optimization/'}, page_content='Fig. 1. The generic framework of teacher-student knowledge distillation training. (Image source: Gou et al. 2020)\\nGiven a dataset, a student model is trained to mimic outputs of a teacher via distillation loss. Usually a neural network has a softmax layer; For example, a LLM outputs a probability distribution over tokens. Let’s denote the logits layer right before softmax as $\\\\mathbf{z}_t$ and $\\\\mathbf{z}_s$ for teacher and student models, respectively. The distillation loss minimizes the difference between two softmax outputs with a high temperature $T$. When ground truth labels $\\\\mathbf{y}$ are known, we can combine it with a supervised learning objective between ground truth and the student’s soft logits using e.g. cross-entropy.\\n\\n$$\\n\\\\mathcal{L}_\\\\text{KD} = \\\\mathcal{L}_\\\\text{distll}(\\\\text{softmax}(\\\\mathbf{z}_t, T), \\\\text{softmax}(\\\\mathbf{z}_s, T)) + \\\\lambda\\\\mathcal{L}_\\\\text{CE}(\\\\mathbf{y}, \\\\mathbf{z}_s)\\n$$'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-01-10-inference-optimization/'}, page_content='Another approach is to consider the full-precision model as the teacher and the lower-precision model as the student, and then optimize the low-precision model with distillation loss. Distillation usually doesn’t need to use the original dataset; E.g. Wikipedia dataset is a good choice and even random tokens can give decent performance gain. The Layer-by-layer Knowledge Distillation (LKD; Yao et al. 2022) method quantizes the network layer by layer and uses its original, unquantized version as the teacher model. Given the same inputs, LKD minimizes the MSE between the multiplication with layer weights and the multiplication of quantized layer weights.\\nPruning#\\nNetwork pruning is to reduce the model size by trimming unimportant model weights or connections while the model capacity remains. It may or may not require re-training. Pruning can be unstructured or structured.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-01-10-inference-optimization/'}, page_content='where $\\\\lambda$ is a hyperparameter to balance between soft and hard learning objectives. A common choice for $\\\\mathcal{L}_\\\\text{distll}$ is KL divergence / cross entropy.\\nA successful early trial is DistilBERT (Sanh et al. 2019) that is able to reduce the parameters of a BERT by 40% while maintaining 97% performance of BERT on fine-tuned downstream tasks and running 71% faster. The loss of pre-training DistilBERT is a combination of soft distillation loss, supervised training loss (i.e. Masked language modeling loss $\\\\mathcal{L}_\\\\text{MLM}$ in the case of BERT) and a special cosine embedding loss to align the hidden state vectors between teacher and student.\\nDistillation can be easily combined with quantization, pruning or sparsification techniques, where the teacher model is the original full-precision, dense model and the student is quantized, pruned, or trimmed to have higher sparsity level.\\nQuantization#')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What is Knowledge distillation?\"\n",
    "docs = retriever.retrieve_documents(question)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Ingredients:\n",
      "- 1 pound ground beef (90% lean)\n",
      "- 1/2 cup breadcrumbs\n",
      "- 1/4 cup grated Parmesan cheese\n",
      "- 1/4 cup chopped fresh parsley\n",
      "- 1/4 teaspoon salt\n",
      "- 1/4 teaspoon black pepper\n",
      "- 8 oz spaghetti (or any other pasta of your choice)\n",
      "- 2 tablespoons olive oil\n",
      "- 2 cloves garlic, minced\n",
      "- 1 large onion, chopped\n",
      "- 1/2 cup tomato sauce\n",
      "- 1 can crushed tomatoes (32 oz)\n",
      "- 1 teaspoon dried oregano\n",
      "- 1/2 teaspoon red pepper flakes (optional)\n",
      "- Freshly grated Parmesan cheese, for serving (optional)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Preheat oven to 375°F.\n",
      "\n",
      "2. In a large bowl, combine ground beef, breadcrumbs, Parmesan cheese, parsley, salt, and pepper. Mix well.\n",
      "\n",
      "3. Divide the mixture into four equal portions, shaping each portion into a ball. Flatten each ball with your hand to form a patty.\n",
      "\n",
      "4. Heat olive oil in a large skillet over medium-high heat. Add spaghetti and cook for 2-3 minutes per side or until golden brown. Remove from the pan and set aside.\n",
      "\n",
      "5. In the same skillet, add garlic and onion and sauté for 1-2 minutes or until softened.\n",
      "\n",
      "6. Add tomato sauce, crushed tomatoes, or\n"
     ]
    }
   ],
   "source": [
    "res = generator.generate_text(\"Here is my grandmother's beloved recipe for spaghetti and meatballs:\")\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
